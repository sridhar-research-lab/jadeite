{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import progressbar #pip install progressbar2\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_ben = pd.read_csv('results_test_ben.csv')\n",
    "pd_mal = pd.read_csv('results_test_mal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = ['file','result']\n",
    "df_gt = pd.DataFrame(columns=column)\n",
    "df_jh = pd.DataFrame(columns=column)\n",
    "\n",
    "for i, row in pd_ben.iterrows():\n",
    "    df_gt = df_gt.append({'file':row['file'], 'result':0},ignore_index=True)\n",
    "    df_jh = df_jh.append({'file':row['file'], 'result':row['m_b']},ignore_index=True)\n",
    "\n",
    "for i, row in pd_mal.iterrows():\n",
    "    df_gt = df_gt.append({'file':row['file'], 'result':1},ignore_index=True)\n",
    "    df_jh = df_jh.append({'file':row['file'], 'result':row['m_b']},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(781, 2)\n",
      "(781, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_gt.shape)\n",
    "print(df_jh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.92612137 0.97014925]\n",
      "recall: [0.96694215 0.93301435]\n",
      "fscore: [0.94609164 0.95121951]\n",
      "support: [363 418] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95       363\n",
      "           1       0.97      0.93      0.95       418\n",
      "\n",
      "    accuracy                           0.95       781\n",
      "   macro avg       0.95      0.95      0.95       781\n",
      "weighted avg       0.95      0.95      0.95       781\n",
      "\n",
      "accuracy 0.9487836107554417\n"
     ]
    }
   ],
   "source": [
    "gt = df_gt['result'].tolist()\n",
    "jh = df_jh['result'].tolist()\n",
    "\n",
    "precision, recall, fscore, support = score(gt, jh)\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support),'\\n')\n",
    "\n",
    "print(classification_report(gt, jh))\n",
    "print('accuracy', accuracy_score(gt, jh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jar_loc = ['/home/iobaidat/Desktop/WeightedCNN/Jarhead features/testing_data/benign/',\n",
    "           '/home/iobaidat/Desktop/WeightedCNN/Jarhead features/testing_data/malicious/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97% (98 of 101) |###################### | Elapsed Time: 0:00:01 ETA:   0:00:00"
     ]
    }
   ],
   "source": [
    "column = ['file','gtresult','jh_result']\n",
    "df_jh_part = pd.DataFrame(columns=column)\n",
    "for loc in jar_loc:\n",
    "    onlyfiles = [f for f in listdir(loc) if isfile(join(loc, f))]\n",
    "    #print(len(onlyfiles))\n",
    "    progress = progressbar.ProgressBar()\n",
    "    for file,i in zip(onlyfiles, progress(range(len(onlyfiles)))):\n",
    "        head, tail = os.path.split(file)\n",
    "        string_ = tail.split(\"_\")\n",
    "        index_list = df_jh[df_jh['file'].str.contains(string_[0])==True].index.tolist()\n",
    "\n",
    "        if not index_list:\n",
    "                    print('Not in jarhead',file,' ',index_list)\n",
    "                    continue\n",
    "        #df_jh_part = df_jh_part.append((df_jh.loc[index_list]).iloc[0,:],ignore_index=True)\n",
    "        df_jh_part = df_jh_part.append({'file':(df_jh.loc[index_list]).iloc[0,0],\n",
    "                                        'gtresult':(df_gt.loc[index_list]).iloc[0,1],\n",
    "                                        'jh_result':(df_jh.loc[index_list]).iloc[0,1]},\n",
    "                                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(445, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jh_part.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.95100865 0.85714286]\n",
      "recall: [0.95930233 0.83168317]\n",
      "fscore: [0.95513748 0.84422111]\n",
      "support: [344 101] \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.96      0.96       344\n",
      "          1       0.86      0.83      0.84       101\n",
      "\n",
      "avg / total       0.93      0.93      0.93       445\n",
      "\n",
      "accuracy 0.9303370786516854\n"
     ]
    }
   ],
   "source": [
    "gt = df_jh_part['gtresult'].tolist()\n",
    "jh = df_jh_part['jh_result'].tolist()\n",
    "\n",
    "precision, recall, fscore, support = score(gt, jh)\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support),'\\n')\n",
    "\n",
    "print(classification_report(gt, jh))\n",
    "print('accuracy', accuracy_score(gt, jh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
